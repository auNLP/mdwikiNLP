{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NER Using Word2Vec\n",
    "We will start of by training a fully connected neural network using Word2Vec as input\n",
    "\n",
    "You are free to use the English Word2vec by Google or the Danish by DaNLP. Either is fine. Remember you can load in the models using:\n",
    "\n",
    "```\n",
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "```\n",
    "Remember you can see how to use there Danish word2vecs [here](https://github.com/alexandrainst/danlp/blob/master/docs/models/embeddings.md) \n",
    "\n",
    "The tagged data for english is located in the folder for the class on github and can be parsed using the following function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   doc_n  sent_n,     word  pos   dep    ne\n",
       "0      0        0       EU  NNP  I-NP   ORG\n",
       "1      0        0  rejects  VBZ  I-VP     O\n",
       "2      0        0   German   JJ  I-NP  MISC\n",
       "3      0        0     call   NN  I-NP     O\n",
       "4      0        0       to   TO  I-VP     O"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_n</th>\n      <th>sent_n,</th>\n      <th>word</th>\n      <th>pos</th>\n      <th>dep</th>\n      <th>ne</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>EU</td>\n      <td>NNP</td>\n      <td>I-NP</td>\n      <td>ORG</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>rejects</td>\n      <td>VBZ</td>\n      <td>I-VP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>German</td>\n      <td>JJ</td>\n      <td>I-NP</td>\n      <td>MISC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>call</td>\n      <td>NN</td>\n      <td>I-NP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>to</td>\n      <td>TO</td>\n      <td>I-VP</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "def get_conll_eng():\n",
    "    # read file\n",
    "    with open(\"eng.train.txt\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    def filter_empty_string(t):\n",
    "        return list(filter(lambda x: x, t))\n",
    "\n",
    "    # split it into documents\n",
    "    docs = raw.split(\"-DOCSTART- -X- O O\")\n",
    "    docs = filter_empty_string(docs)\n",
    "    res = []\n",
    "    for n, doc in enumerate(docs):\n",
    "        sents = filter_empty_string(doc.split(\"\\n\\n\")) # split into sentences\n",
    "        for sent_n, sent in enumerate(sents):\n",
    "            token_tags = filter_empty_string(sent.split(\"\\n\")) # split into tokens (w. tags)\n",
    "            for t in token_tags:\n",
    "                word, pos, dep, ne = t.split(\" \") # split into tags and token\n",
    "                if len(s := ne.split(\"-\")) == 2:\n",
    "                    ne = s[1]\n",
    "                res.append((n, sent_n, word, pos, dep, ne))\n",
    "    df = pd.DataFrame(res, columns=\"doc_n sent_n, word pos dep ne\".split()) # return as a df\n",
    "    return df\n",
    "\n",
    "df = get_conll_eng()\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "For Danish you can use this code instead (I would collapse the B-\\* or I-\\* as these denote Begining (B) of a named entity and end or inside (I) of a named entity):\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentence_id      words labels\n",
       "0            0         På      O\n",
       "1            0     fredag      O\n",
       "2            0        har      O\n",
       "3            0        SID  B-ORG\n",
       "4            0  inviteret      O"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>words</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>På</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>fredag</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>har</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>SID</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>inviteret</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from danlp.datasets import DDT\n",
    "import pandas as pd\n",
    "\n",
    "def get_conll_da():\n",
    "    # Loading the Danish Dependency Tree data\n",
    "    ddt = DDT()\n",
    "    conllu_format = ddt.load_as_conllu(predefined_splits = True)\n",
    "\n",
    "    data = []\n",
    "    for n in range(len(conllu_format)):\n",
    "        data.append([(i, token.form, token.misc.get(\"name\").pop()) for i, sent in enumerate                 (conllu_format[n]) for token in sent]) #Getting the sentence #, Word and Tag.\n",
    "\n",
    "    train = pd.DataFrame(data[0], columns = ['sentence_id', 'words', 'labels'])\n",
    "    return train\n",
    "\n",
    "df = get_conll_da()\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "# Preprocessing\n",
    "We will start of by determining the output category you can use the following code the get categorical output vectors (as one hot)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'O': 0, 'B-ORG': 1, 'B-LOC': 2, 'B-PER': 3, 'I-PER': 4, 'B-MISC': 5, 'I-LOC': 6, 'I-MISC': 7, 'I-ORG': 8}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "import numpy as np\n",
    "def col_to_onehot(col):\n",
    "    \"\"\"\n",
    "    turn column to one hot\n",
    "    \"\"\"\n",
    "    preds = list(col.unique())\n",
    "    n_predictions = len(preds)\n",
    "    preds_one_hot = {pred: i for pred, i in zip(preds, range(n_predictions))}\n",
    "\n",
    "    a = np.array(col.apply(lambda x: preds_one_hot[x]))\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return preds_one_hot, b\n",
    "\n",
    "preds_one_hot, y = col_to_onehot(df[\"labels\"])\n",
    "print(preds_one_hot) # Again collapsing I-* and B-* is ideal\n",
    "y[:10]"
   ]
  },
  {
   "source": [
    "## Exercises\n",
    "\n",
    "1) Collapse the I-\\* and B-\\* in labels (if using Danish)\n",
    "2) Create a matrix consisting of the word embeddings with the shape (n_samples, embedding_size) the following function might give you a hint of where to start\n",
    "```\n",
    "    def get_embed(x):\n",
    "        if x in w2v:\n",
    "            return w2v[x]\n",
    "        else:\n",
    "            return w2v[\"UNK\"]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Creating a Fully connected Neural Network:\n",
    "Here we will start of doing a neural Network\n",
    "\n",
    "## Exercises\n",
    "1) Start of by making a one layer Neural Network,you can use the following code as inspiration, I recommend that you start with an input layer where the input shape is equal to the embedding size and end with fully connected (dense) layer with size equal to the number of predictions  \n",
    "\n",
    "2) Train the model using at least 10 epochs. While it is training examine why one would use the categorical cross entropy loss vs. the mean squared error (MSE) <-- which you know from linear regression\n",
    "\n",
    "3) Check if the model performs as intended consider the code in block 3 an inspiration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)  # remember to run of in eager mode otherwise it will throw an error\n",
    "\n",
    "# Make model\n",
    "model = tf.keras.Sequential()\n",
    "# add layers\n",
    "model.add(...)\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")\n",
    "model.summary() # inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X.shape, y, validation_split=0.1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3( Evaluate Performance\n",
    "input_embedding = tf.constant(gensim_w2v[\"Kenneth\"])\n",
    "x = tf.reshape(input_embedding, shape=(1, 300))  # desired shape (1 is batch size)\n",
    "model.predict(x).round() # make prediction"
   ]
  },
  {
   "source": [
    "# LSTM\n",
    "LSTM or long short-term memory networks was used for a long time prior to the Transformer architecture and is today still used robotics and translation tasks. Here we will implement it in Keras. Note this is intended to be challenging, but not undoable.\n",
    "\n",
    "## Exercises\n",
    "1) We will have to change the input. Instead of being input vectors each sentence should be series of letter e.g. `[1, 28, 302]`, corresponding to their index in the word2vec embedding you can use the code in the first block for this. Similarly the output should be a sequence of predictions for each token.\n",
    "\n",
    "2) You will need to replace the input layer in the model above with embedding layer in block 2. This layer simply applied the weight from gensims word2vec to each of the numbers and makes sure they are input correctly to any recurrent layers\n",
    "\n",
    "3) Replace dense layers in the previous model (except the last) with the LSTM module by keras, remember to set `return_sequences=True` and consider why you set it to true (feel free to ask me if you can't figure it out)\n",
    "\n",
    "4) Lastly you should wrap the last layer in a the `TimeDistributed` ([ref](https://keras.io/api/layers/recurrent_layers/time_distributed/)) as to secure output is proberly matched up in the sequence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "index_word = genism_w2v.index2word\n",
    "word_to_index = {word: i for i, word, in enumerate(index_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) make embedding layer\n",
    "embedding_layer = gensim_w2v.wv.get_keras_embedding(train_embeddings=False)"
   ]
  }
 ]
}