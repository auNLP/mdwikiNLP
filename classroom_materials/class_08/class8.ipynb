{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('NLP': venv)",
   "display_name": "Python 3.8.5 64-bit ('NLP': venv)",
   "metadata": {
    "interpreter": {
     "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Topic Modelling\n",
    "Plan:\n",
    "- Brief introduction to tagging software for exam projects\n",
    "  - Including text classification and sequence labeling\n",
    "  - Discuss results obtained in the guide and any potential issues (short)\n",
    "- **Examine how topic models are trained**\n",
    "- Examining topics of topic models\n",
    "- Application of topic models\n",
    "- (15-30 minutes guest talk)\n",
    "\n",
    "---\n",
    "# The general Idea\n",
    "<img src=\"pics/general_idea.png\" width=\"500\"/>\n",
    "\n",
    "So assuming we have some data we will need a generative model - i.e. our assumptions about the data generating process. Let's start with the first simple ideas:\n",
    "\n",
    "<img src=\"./pics/idea1.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"./pics/idea2.png\" width=\"600\"/>\n",
    "\n",
    "## What we want\n",
    "<img src=\"./pics/what_we_want.png\" width=\"600\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Directed Acylic Graph ðŸŽ‰\n",
    "Or graphical model\n",
    "\n",
    "<img src=\"./pics/dag2.png\" width=\"600\"/>\n",
    "\n",
    "- Nodes are random variables; edges indicate dependence.\n",
    "- Shaded nodes are observed; unshaded nodes are hidden.\n",
    "- Plates indicate replicated variables.\n",
    "\n",
    "(You will get more on this next semester)\n",
    "\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$\\theta$ is the topic distribution for the document $M$\n",
    "\n",
    "$N$ is number of words\n",
    "\n",
    "$w$ is a word\n",
    "\n",
    "$z$ is a topic\n",
    "\n",
    "$\\alpha$ and $\\beta$ is a hyperparameter\n",
    "\n",
    "\n",
    "\n",
    "## Alternate Explanation\n",
    "\n",
    "LDA assumes the following generative process for each document $\\mathbf{w}$ in a corpus $D$:\n",
    "\n",
    "1. Choose $N \\sim$ Poisson$(\\xi)$ (i.e. choose number of words in the text)\n",
    "2. Choose $\\theta \\sim \\operatorname{Dir}(\\alpha)$ (i.e. choose your distribution of topics)\n",
    "3. For each of the $N$ words $w_{n}$:\n",
    "\n",
    "    (a) Choose a topic $z_{n} \\sim$ Multinomial$(\\theta)$.\n",
    "\n",
    "    (b) then choose a word $w_{n}$ from $p\\left(w_{n} \\mid z_{n}, \\beta\\right),$ a multinomial probability conditioned on the topic $z_{n}$\n",
    "\n",
    "---\n",
    "# Infer the Hidden Parameters\n",
    "We will not do this (ðŸ˜¢), but you will learn how you could do this next semester in cognitive modelling.\n",
    "\n",
    "but here is some algorithms to approximate posteriors:\n",
    "- **Mean field variational methods (Blei et al., 2001,2003)** <-- original\n",
    "- Expectation propagation (Minka and Lafferty, 2002)\n",
    "- **Collapsed Gibbs sampling (Griffiths and Steyvers, 2002)** <-- most similar to what you will do next semester\n",
    "- Distributed sampling (Newman et al., 2008; Ahmed et al., 2012)\n",
    "- Collapsed variational inference (Teh et al., 2006)\n",
    "- **Online variational inference (Hoffman et al., 2010)** <-- most similar to what gensim does\n",
    "- Factorization based inference (Arora et al., 2012; Anandkumar et al., 2012)\n",
    "\n",
    "# What about those Hyperparameters?\n",
    "<img src=\"./pics/a1.png\" width=\"300\"/>\n",
    "<img src=\"./pics/a2.png\" width=\"300\"/>\n",
    "<img src=\"./pics/a3.png\" width=\"300\"/>\n",
    "\n",
    "x-axis is topic. Each image is a document. Thus higher $\\alpha$ leads to more uniform distribution and lower $\\alpha$ leads to each document consisting of only few topics\n",
    "\n",
    "$\\beta$ is similar but for words to topics instead of topics to documents. i.e. lower $\\beta$ leads to topics with only a few dominant words.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "Plan:\n",
    "- Brief introduction to tagging software for exam projects\n",
    "  - Including text classification and sequence labeling\n",
    "  - Discuss results obtained in the guide and any potential issues (short)\n",
    "- Examine how topic models are trained\n",
    "- **Examining topics of topic models**\n",
    "- Application of topic models\n",
    "\n",
    "---\n",
    "##  Examining and Interpreting Topics\n",
    "\n",
    "Visualisation using LDAvis ([ref](https://ldavis.cpsievert.me/reviews/vis/#topic=6&lambda=0.6&term=)) \n",
    "\n",
    "\n",
    "<img src=\"./pics/topics.png\" width=\"700\"/>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "Plan:\n",
    "- Brief introduction to tagging software for exam projects\n",
    "  - Including text classification and sequence labeling\n",
    "  - Discuss results obtained in the guide and any potential issues (short)\n",
    "- Examine how topic models are trained\n",
    "- Examining topics of topic models\n",
    "- **Application of topic models**\n",
    "---\n",
    "##  Application\n",
    "What could you imagine are some applications of topic models?\n",
    "\n",
    "```\n",
    "if Studygroup.size > 3:\n",
    "    group_1, group_2 = Studygroup.split()\n",
    "    group_1.discuss(question)\n",
    "    group_2.discuss(question)\n",
    "else:\n",
    "    Studygroup.discuss(question)\n",
    "```\n",
    "\n",
    "<!--\n",
    "- Information retrieval (recommender system)\n",
    "- Corpus overview\n",
    "- Input in downstream task\n",
    "    - classification\n",
    "    - q&a\n",
    "    - chatbots\n",
    "- Quality check (404 errors theme in online corpora)\n",
    "- Spam filters\n",
    "- Clustering of bioinformatics data\n",
    "- Large scale text analysis (e.g. exploring the great unread - [ref](https://www.sciencedirect.com/science/article/pii/S0304422X13000648))\n",
    "\n",
    "## Example 1:  Change in topic over time\n",
    "In topic usage:\n",
    "\n",
    "<img src=\"./pics/topic_over_time.png\" width=\"700\"/>\n",
    "\n",
    "And in a topic itself:\n",
    "\n",
    "<img src=\"./pics/tot1.png\" width=\"700\"/>\n",
    "\n",
    "Simply by allowing the beta to vary accross time:\n",
    "\n",
    "<img src=\"./pics/tot_model.png\" width=\"700\"/>\n",
    "\n",
    "# Example 2: Scholarly impact\n",
    "\n",
    "<img src=\"./pics/impact_model.png\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./pics/impact1.png\" width=\"700\"/>\n",
    "\n",
    "-->"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 0: Discuss with group - could you and how could you use topic modelling in your project? \n",
    "# exercise 1: what does it do when you change the hyperparameter of X\n",
    "# exercise 2: What is perplexity? why it is a good selection criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}